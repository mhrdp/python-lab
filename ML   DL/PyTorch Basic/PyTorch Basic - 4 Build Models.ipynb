{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Basic - 4 Build Models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMG4RJ8CPyBTmf2syELY335"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Usv__ZebsKsK"},"source":["# Build The Neural Network\n","Article: [here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n","\n","Neural networks **comprise of layers/modules that perform operations on data**. The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n","\n","In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset."]},{"cell_type":"code","metadata":{"id":"JB8kIC3zr93I","executionInfo":{"status":"ok","timestamp":1629683555296,"user_tz":-420,"elapsed":4899,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}}},"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYTC_Gq_tc8Y"},"source":["## Get Device For Training\n","We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let’s check to see if `torch.cuda` is available, else we continue to use the CPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEnqxlh8tvWx","executionInfo":{"status":"ok","timestamp":1629683803180,"user_tz":-420,"elapsed":514,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"d67c8d67-105b-4610-87b1-db848eb13449"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using {device} device.')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using cpu device.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AA1UdlfquamC"},"source":["## Define The Class\n","We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the forward method."]},{"cell_type":"code","metadata":{"id":"dyfbKdYbuox_","executionInfo":{"status":"ok","timestamp":1629683897582,"user_tz":-420,"elapsed":509,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}}},"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3R-n7yr_uwMY"},"source":["We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bkt7qY1NvHlS","executionInfo":{"status":"ok","timestamp":1629684019004,"user_tz":-420,"elapsed":497,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"b3759ae8-a86a-47ad-8134-b9abd785429f"},"source":["model = NeuralNetwork().to(device)\n","print(model)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V0WyzVz7vXlC"},"source":["To use the model, we pass it the input data. This executes the model’s `forward`, along with some background operations. Do not call `model.forward()` directly!\n","\n","**Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class**. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g15-Bjegvusu","executionInfo":{"status":"ok","timestamp":1629684192769,"user_tz":-420,"elapsed":501,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"7fa5b296-9870-4df8-8a56-8847ff02cbe8"},"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Predicted class: tensor([6])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wd_eH2XXv6k-"},"source":["## Model Layers\n","Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXPrX1UtwBI2","executionInfo":{"status":"ok","timestamp":1629684251570,"user_tz":-420,"elapsed":498,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"b3c5d66b-fd55-4089-acb5-b94613f2bb9f"},"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["torch.Size([3, 28, 28])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C16CblCnwFHQ"},"source":["### nn.Flatten\n","We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ReJ3_WIwPkM","executionInfo":{"status":"ok","timestamp":1629684312493,"user_tz":-420,"elapsed":499,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"e4797dda-430f-496f-c46f-e70b3deb8800"},"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["torch.Size([3, 784])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dkQQQjhiwyP5"},"source":["### nn.Linear\n","The linear layer is a module that applies a linear transformation on the input using its stored weights and biases."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gQfx0adw6yJ","executionInfo":{"status":"ok","timestamp":1629684488664,"user_tz":-420,"elapsed":525,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"36082759-7c93-49a9-9c16-a384c000de0f"},"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["torch.Size([3, 20])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tLpxtDmXw_i0"},"source":["### nn.ReLU\n","Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n","\n","In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there’s other activations to introduce non-linearity in your model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWm3TLVXxX43","executionInfo":{"status":"ok","timestamp":1629684607338,"user_tz":-420,"elapsed":493,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"fab43bf5-2d74-4121-db21-54afb7baeea7"},"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Before ReLU: tensor([[ 0.3335,  0.5813, -0.1959, -0.3181, -0.4606, -0.3562, -0.1151, -0.2908,\n","          0.2991,  0.5221,  0.1389,  0.3689, -0.1902, -0.4460, -0.1666, -0.0840,\n","          0.2084, -0.5358,  0.4481, -0.0888],\n","        [ 0.6632,  0.5283, -0.4120, -0.5511, -0.2641, -0.0018, -0.2459, -0.2074,\n","         -0.1146,  0.7517,  0.3059,  0.1360, -0.2241, -0.3536, -0.2870, -0.0490,\n","          0.1496, -0.4603,  0.3524, -0.3919],\n","        [ 0.4752,  0.3665, -0.3735, -0.3982, -0.2287, -0.2470, -0.2425, -0.2245,\n","          0.0572,  1.0148, -0.0491,  0.2972, -0.1830, -0.4393, -0.2371, -0.4527,\n","          0.2392, -0.3158, -0.0375, -0.3758]], grad_fn=<AddmmBackward>)\n","\n","\n","After ReLU: tensor([[0.3335, 0.5813, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2991,\n","         0.5221, 0.1389, 0.3689, 0.0000, 0.0000, 0.0000, 0.0000, 0.2084, 0.0000,\n","         0.4481, 0.0000],\n","        [0.6632, 0.5283, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.7517, 0.3059, 0.1360, 0.0000, 0.0000, 0.0000, 0.0000, 0.1496, 0.0000,\n","         0.3524, 0.0000],\n","        [0.4752, 0.3665, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0572,\n","         1.0148, 0.0000, 0.2972, 0.0000, 0.0000, 0.0000, 0.0000, 0.2392, 0.0000,\n","         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EVqlxWCDxf_T"},"source":["### nn.Sequential\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."]},{"cell_type":"code","metadata":{"id":"SL_aucRBxsgG","executionInfo":{"status":"ok","timestamp":1629684824052,"user_tz":-420,"elapsed":496,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}}},"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJgP6oiAyRST"},"source":["### nn.Softmax\n","The last linear layer of the neural network returns logits - raw values in `[-infty, infty]` - which are passed to the `nn.Softmax` module. The logits are scaled to values `[0, 1]` representing the model’s predicted probabilities for each class. `dim` parameter indicates the dimension along which the values **must sum to 1**."]},{"cell_type":"code","metadata":{"id":"wjztx-6nysyQ","executionInfo":{"status":"ok","timestamp":1629684953484,"user_tz":-420,"elapsed":718,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}}},"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIMBOgtlyweZ"},"source":["## Model Parameter\n","Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.\n","\n","In this example, we iterate over each parameter, and print its size and a preview of its values."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccy1Lzl3y9Ps","executionInfo":{"status":"ok","timestamp":1629685023349,"user_tz":-420,"elapsed":771,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"bcb7077f-8983-430e-b9aa-36e5d7b190a2"},"source":["print(\"Model structure: \", model, \"\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model structure:  NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",") \n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0135, -0.0081,  0.0356,  ..., -0.0053, -0.0336, -0.0082],\n","        [ 0.0211, -0.0342,  0.0108,  ...,  0.0152, -0.0243,  0.0041]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0239,  0.0288], grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0426, -0.0110, -0.0279,  ...,  0.0027,  0.0144,  0.0252],\n","        [-0.0408, -0.0242, -0.0356,  ...,  0.0170,  0.0393,  0.0166]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0061, -0.0401], grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0021,  0.0403,  0.0427,  ..., -0.0215, -0.0325, -0.0425],\n","        [-0.0263,  0.0182,  0.0306,  ...,  0.0159,  0.0275,  0.0067]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0329,  0.0014], grad_fn=<SliceBackward>) \n","\n"],"name":"stdout"}]}]}