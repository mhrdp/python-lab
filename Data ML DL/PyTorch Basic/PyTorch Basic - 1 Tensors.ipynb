{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Basic - 1 Tensors.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPqEvYHLw3eSXNKq5zt0Knc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W1ZQcAWPIkGd"},"source":["# Tensors\n","Tensors are a **specialized data** structure that are **very similar to arrays and matrices**. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n","\n","Tensors are similar to NumPy’s *ndarrays*, except that **tensors can run on GPUs or other hardware accelerators**. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see [Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the [Autograd](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label) section). If you’re familiar with *ndarrays*, you’ll be right at home with the Tensor API. If not, follow along!"]},{"cell_type":"code","metadata":{"id":"fYm9mazOIZ5G"},"source":["import torch\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NJZwksFKJUR"},"source":["## Initializing a Tensor\n","Tensor can be initialized in various ways."]},{"cell_type":"markdown","metadata":{"id":"uuW2l40mKXLw"},"source":["### Directly from the data\n","Tensors can be created directly from the data. In this case, data `type` will be automatically referred."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flFBfUXqKo4I","executionInfo":{"status":"ok","timestamp":1629445827335,"user_tz":-420,"elapsed":28,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"ce731008-2df8-4e83-9d94-7267d6ccff69"},"source":["data = [[1, 2], [3, 4]]\n","x_data = torch.tensor(data)\n","\n","print('data variable:', data, '| type:', type(data))\n","print('x_data variable:', x_data, '| type:', type(x_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data variable: [[1, 2], [3, 4]] | type: <class 'list'>\n","x_data variable: tensor([[1, 2],\n","        [3, 4]]) | type: <class 'torch.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2BRIXwffLS7s"},"source":["### From NumPy Array\n","Tensors can be created from NumPy arrays (and vice versa - see [Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label))."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zfy9YD60LdQ8","executionInfo":{"status":"ok","timestamp":1629445827337,"user_tz":-420,"elapsed":22,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"2c29607a-2463-4262-d388-5d6562ec0ed1"},"source":["np_array = np.array(data)\n","x_np = torch.tensor(np_array)\n","\n","print('np_array variable:', np_array, '| type:', type(np_array))\n","print('x_np variable:', x_np, '| type:', type(x_np))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["np_array variable: [[1 2]\n"," [3 4]] | type: <class 'numpy.ndarray'>\n","x_np variable: tensor([[1, 2],\n","        [3, 4]]) | type: <class 'torch.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"01We91L6MF2q"},"source":["### From Another Tensor\n","The new tensor will **retain** the properties (shapes, datatype) of the argument tensor, unless explicitly overridden. The data, however, will be completely new.\n","\n","**Notes**: `dtype` is datatype"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emUqYwG_Mkoa","executionInfo":{"status":"ok","timestamp":1629445827888,"user_tz":-420,"elapsed":567,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"77ab60f3-fea2-47f9-8ca0-8ee76f86a2dd"},"source":["# Will retain the properties\n","x_ones = torch.ones_like(x_data)\n","\n","\n","# Will be overrides the datatype of x_data\n","x_rand = torch.rand_like(x_data, dtype=torch.float)\n","\n","print('Ones tensor: \\n', x_ones, '\\n')\n","print('Random Tensor: \\n', x_rand)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ones tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.6158, 0.3911],\n","        [0.7273, 0.8181]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ivIEvNDN-mt"},"source":["### With Random or Constant Value\n","`shape` is a tuple of tensor dimensions. In the function below, it determines the dimensionality of the output tensor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhxAqaoLOW_M","executionInfo":{"status":"ok","timestamp":1629445827891,"user_tz":-420,"elapsed":25,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"a8f77dff-319f-4aca-c31e-8718d722b148"},"source":["shape = (2, 3)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape, dtype=torch.int)\n","zeros_tensor = torch.zeros(shape)\n","\n","print('Random tensor:\\n', rand_tensor)\n","print('Ones tensor:\\n', ones_tensor)\n","print('Zeros tensor:\\n', zeros_tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random tensor:\n"," tensor([[0.9356, 0.8515, 0.1594],\n","        [0.8874, 0.4868, 0.2271]])\n","Ones tensor:\n"," tensor([[1, 1, 1],\n","        [1, 1, 1]], dtype=torch.int32)\n","Zeros tensor:\n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SQdEkmCgPMIC"},"source":["## Attributes of Tensor\n","Tensor attributes describe their shape, datatype, and the device on which they are stored"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4O3nQYRwPSIk","executionInfo":{"status":"ok","timestamp":1629445827894,"user_tz":-420,"elapsed":24,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"7ea54d7c-7750-42f9-d8d2-3a1b29cefd9e"},"source":["tensor = torch.rand(3, 4)\n","print(f'Shape of tensor: {tensor.shape}')\n","print(f'Datatype of tensor: {tensor.dtype}')\n","print(f'Devoce tensor is stored on: {tensor.device}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Devoce tensor is stored on: cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aIXwnyvQPwZ3"},"source":["## Operations on Tensor\n","Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html).\n","\n","Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). **If you’re using Google Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.**\n","\n","By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to` method (after checking for GPU availability). **Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!**"]},{"cell_type":"code","metadata":{"id":"0Xh-AbHWQWwn"},"source":["# Move operations to GPU if available\n","if torch.cuda.is_available():\n","    tensor = tensor.to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J168uQrbQvLd"},"source":["Try out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.\n","\n","### Standart NumPy-like indexing and slicing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGXDgsj9Q6cn","executionInfo":{"status":"ok","timestamp":1629445856820,"user_tz":-420,"elapsed":345,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"33a17e13-637c-4971-b116-c206524ec499"},"source":["tensor = torch.ones(4, 4)\n","\n","print('First row: ', tensor[0])\n","print('First column: ', tensor[:, 0])\n","print('Last column:', tensor[..., -1])\n","\n","tensor[:, 1] = 0\n","print(tensor)\n","print(tensor[1, -1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First row:  tensor([1., 1., 1., 1.])\n","First column:  tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x0gJyNKShSiZ"},"source":["### Joining Tensors\n","You can use `tensor.cat` to concatenate a sequence of tensors along a given dimension. See also [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack.html), another tensor joining op that is subtly different from `torch.cat`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-UuSETBhqzp","executionInfo":{"status":"ok","timestamp":1629445879472,"user_tz":-420,"elapsed":384,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"690facc6-de2d-4dde-ee09-f989e0d749bd"},"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EHsy3L3fkRpF"},"source":["### Arithmetic Operation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQZpzJ4SkXrM","executionInfo":{"status":"ok","timestamp":1629447308427,"user_tz":-420,"elapsed":360,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"0fa37367-77ca-4efa-b966-7c87d7e4d692"},"source":["# This computes the matrix multiplications between two tensors. y1, y2, y3 will have the same value\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","y3 = torch.rand_like(tensor)\n","print(f'y1: {y1}')\n","print(f'y2: {y2}')\n","print(f'y3: {y3}')\n","print(f'torch.matmul: {torch.matmul(tensor, tensor.T, out=y3)}\\n')\n","\n","# This computes the element-wise product. z1, z2, z3 will have the same value\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","z3 = torch.rand_like(tensor)\n","print(f'z1: {z1}')\n","print(f'z2: {z2}')\n","print(f'z3: {z3}')\n","print(f'torch.mul: {torch.mul(tensor, tensor, out=z3)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["y1: tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n","y2: tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n","y3: tensor([[0.0021, 0.0093, 0.4824, 0.0660],\n","        [0.3611, 0.8822, 0.4078, 0.9826],\n","        [0.1399, 0.8871, 0.3738, 0.8707],\n","        [0.1428, 0.8904, 0.4971, 0.8383]])\n","torch.matmul: tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n","\n","z1: tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","z2: tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","z3: tensor([[0.3280, 0.8682, 0.0657, 0.3915],\n","        [0.0731, 0.6078, 0.5044, 0.9709],\n","        [0.8445, 0.3832, 0.1441, 0.4772],\n","        [0.5930, 0.2461, 0.9264, 0.4297]])\n","torch.mul: tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"giY9FWWfoURI"},"source":["### Single-element tensors\n","If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKuH2GwCoarj","executionInfo":{"status":"ok","timestamp":1629447456353,"user_tz":-420,"elapsed":330,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"666bfe9e-e86b-4833-f839-e079d7ad6459"},"source":["agg = tensor.sum()\n","agg_item = agg.item()\n","\n","# Without .item()\n","print(agg, type(agg))\n","\n","# With .item()\n","print(agg_item, type(agg_item))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(12.) <class 'torch.Tensor'>\n","12.0 <class 'float'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qRgsmJx3uPep"},"source":["### In-place operations\n","Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`, `x.t_()`, will change `x`.\n","\n","**IMPORTANT NOTE**: *In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1CUh_W5uZW7","executionInfo":{"status":"ok","timestamp":1629449009946,"user_tz":-420,"elapsed":330,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"4e1007d6-a806-4b0b-d2c8-13ba90592496"},"source":["print(f'Before: {tensor}, \\n')\n","tensor.add_(5)\n","print(f'After: {tensor}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before: tensor([[6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.]]), \n","\n","After: tensor([[11., 10., 11., 11.],\n","        [11., 10., 11., 11.],\n","        [11., 10., 11., 11.],\n","        [11., 10., 11., 11.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PXEGLioHu6tH"},"source":["## Bridge with NumPy\n","Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."]},{"cell_type":"markdown","metadata":{"id":"HGiL-aZcvAA3"},"source":["### Tensor to NumPy Array\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_V6EwK9vLkG","executionInfo":{"status":"ok","timestamp":1629449153698,"user_tz":-420,"elapsed":369,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"0e704430-be46-4a49-b2b1-b8e53cf2c67d"},"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","\n","n = t.numpy()\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jKtMEBvCvUrO"},"source":["A change in the tensor reflects in the NumPy array"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQWIC8sJvZ70","executionInfo":{"status":"ok","timestamp":1629449215942,"user_tz":-420,"elapsed":356,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"822abaef-6bd2-4f16-80ae-505388f2950a"},"source":["t.add_(1)\n","\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PEN_4jmWvh0L"},"source":["### NumPy array to tensor"]},{"cell_type":"code","metadata":{"id":"9Igs79jgvnxw"},"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yn8oU3N5vrqv"},"source":["Changes in the NumPy array reflects in the tensor."]},{"cell_type":"code","metadata":{"id":"1oOMrE7YvsyU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629449285675,"user_tz":-420,"elapsed":369,"user":{"displayName":"Mahardhika Dwi","photoUrl":"","userId":"18064939782869251856"}},"outputId":"ff1d1c88-e750-4198-c7b0-29193b028ba8"},"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"],"name":"stdout"}]}]}